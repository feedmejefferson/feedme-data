---
title: "Ranking Food Popularity"
output: html_notebook
---

## Access Logs

Before we can do anything with the data, we need to import the raw access logs and convert them into a format more usable in R. We've already created a reuseable script for this, so we'll just source it to build our initial `access_log` dataframe.

```{r, message=FALSE}
source("prep-data.R")
summary(access_log)
```

## Pivoting the Data

Right now, the bulk of the data that we're interested in sits in the query string parameters. In order to see which images of food are getting clicked on and which ones aren't getting clicked on (as a basic measurement of popularity), we'll transform the input dataframe into a new dataframe with one entry for every image chosen and another for every image not chosen. For now we'll assign weights of 1 and -1. 


```{r}
library(reshape2)
melted <- melt(access_log, 
               id.vars=c("search.session"),
               measure.vars=c("chosen", "not.chosen"))

## assign chosen and not chosen positive and negative weightings
map <- c(-1,1)
names(map) <- c("not.chosen","chosen")
melted$weight <- map[as.character(melted$variable)]
```

Next we'll reshape that into a wide dataframe with one column for each image, summing up all of the weights for each image and separately counting the number of times each image was seen. Due to the random selection of images, in our small dataset, some images will arbitrarily have been seen more than others (and some will never have been seen). 

I'm sure that there's a better statistical/probability based calculation for scoring popularity of images, but for now I'm simply going to divide the sum of the clicks minus not clicks and divide that by one plus the number of times that the image was seen. Adding one will give higher weighted scores to images that were clicked seven out of seven times than only once out of one time. 

```{r}
counts <- dcast(melted, "all" ~ value, fun.aggregate = length, value.var="weight")
ratings <- dcast(melted, "all" ~ value, fun.aggregate = sum, value.var="weight")
weighted.ratings <- ratings[,-1] / (counts[,-1] + 1)
```

## Basic Popularity Rankings

Finally, we can order the images by their weighted ranking and pick the top and bottom 10 chosen foods to see if they make sense. Since we sourced the images from random pictures of food on the internet (specifically free stock photos with a creative commons zero license provided by pexels.com), many of them aren't really appropriate for our purposes. 

We would likely expect to find that the images that are the least popular are also the best candidates for removing from our pool of images. 

```{r}
ordered.ratings <- weighted.ratings[order(weighted.ratings[1,])]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```

### Popular Foods

```{r}
library(knitr)
## Note, this assumes that the images are sitting in a folder of the parent 
## directory named images
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Unpopular Foods


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```

## Randomness

For now we've just been showing two random options in response to every hunger request. The hunger app doesn't keep track of what it's shown, so it makes no effort, nor has any ability to prevent repeat images within the same session. Sometimes when clicking through images, it seems like some foods come up more frequently. Is this just our mind playing tricks on us, showing us patterns in the randomness? Or is the randomness that we rely on less than random? If the distribution truly is randomly uniform, then we would expect any particular sample to be less than perfectly uniform, but over time we would expect larger sample sizes to be very uniform. How are we doing?


```{r}
library(dplyr)
data <- select(melted, value, variable) %>%
  rename(., image.name = value, selection = variable) %>%
  group_by(image.name, selection)  %>% 
  summarise(hits=n())
data.wide <- dcast(data, image.name ~ selection)
data.wide$shown <- data.wide$chosen + data.wide$not.chosen
hist(data.wide$shown, breaks=20)
```
If this sample was truly coming from a uniform distribution, then we would expect the histogram of the samples to look something like a normal bell curve -- where the curve gets tighter and tighter around the center as the population size of the sample increases. Our histogram looks something like that, but it's a bit skewed to the left. This is probably because we removed a number of images from the site that are still showing up in the data. The counts for those images won't increase anymore, so effectively we're seeing two bell curves stacked on top of each other. One for the sample of a images selected uniformly randomly from the original set of images, and then a second sample of images selected uniformly randomly from the later pruned set of images. 

We can remove those images from our data though:

```{r}
removed.images <- c("0000007.jpg","0000011.jpg","0000057.jpg","0000086.jpg","0000099.jpg","0000132.jpg","0000133.jpg","0000145.jpg","0000206.jpg","0000230.jpg","0000236.jpg","0000239.jpg","0000240.jpg","0000242.jpg","0000245.jpg","0000258.jpg","0000283.jpg","0000286.jpg","0000289.jpg","0000315.jpg","0000318.jpg","0000363.jpg","0000364.jpg","0000374.jpg")
data.wide <- filter(data.wide, !image.name %in% removed.images)
hist(data.wide$shown, breaks=20)
```

This doesn't really seem to make the skew any better though...

Finally, if we just want to see how often images were chosen or not chosen ordered by how frequently they were shown, here's a simple visual:

```{r, fig.width=10,fig.height=30}
ggplot(data.wide, aes(reorder(image.name,shown))) + geom_col(aes(y=shown,fill="shown")) + geom_col(aes(y=chosen,fill="chosen")) + coord_flip()
```