---
title: "Access Log Analysis"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

This notebook demonstrates how we can run some simple analytics on the access log data that we collect from the feedme jefferson site and generate automatic reports that help us understand and visualize the significance of the data we've collected. 

## Importing and Normalizing the access log

Before we can do anything with the data, we need to import the raw access logs and convert them into a format more usable in R. The `prep-data.R` script that we source to do this reads in any files matching the glob `access_log.*`. We roll the access log files on the server on a daily basis and gzip them for analysis here. 

```{r, message=FALSE}
source("prep-data.R")
```

We can use the `summary` function to get a quick summary of what's in our logs.

```{r}
summary(access_log)
```

## Clicks by day

Hits or clicks are a pretty typical metric to look at. We don't care so much about visits in general, mostly we're interested in how many choices we captured (so specifically hits to the `hunger.json` path and more specifically, ones that have a chosen/notChosen query param). 

It would be nice to see the relationship between both people (who the hits are coming from) and sessions (how many clicks do we tend to get for a specific appetite). For now we'll just use user agent as a way of identifying distinct people. 

```{r}
library(dplyr)
library(ggplot2)
access_log$date <- as.Date(access_log$datetime)
data <- select(access_log, date, useragent, search.session) %>%
  group_by(date, useragent, search.session)  %>% 
  summarise(hits=n())
data$user.short<-factor(paste(as.integer(data$useragent),substr(data$useragent,14,25)))
ggplot(data, aes(x=date)) +
  geom_bar(aes(weight=hits, fill=user.short))
```

The above bar chart shows clicks by date with the clicks from each visitor/user showing up in a different color. We can see a lot of clicks from the first days worth of data (before we actually started hosting anything publicly) followed by a break in the data, followed by an initial spike (after we told our friends to take a look) followed by a bit of a drop off. Since then most of our clicks are coming from the same visitor -- that would be Karen, after all, we are building this for her and need her data if we're going to map out her food preferences!

```{r}
ggplot(data, aes(date, hits)) +
  geom_point(aes(y=hits, col=user.short))
```
In the above plot, we drill down a bit further to look at session sizes. Rather summing up all of the sessions for a user and stacking them in a bar chart, here we're showing a single point for each session so that we can differentiate between the same person clicking 400 images in a row, or 200 images at two different times in the day. We can see that the size of sessions varies quite a bit. Most sessions involve less than 100 clicks but some go much longer. We should take this into consideration when running PCA on our search sessions -- how do we want to normalize/weight the data? Should longer sessions that contain more data contribute more weighting or the same as smaller sessions? Should a click contribute the same amount of information regardless the size of the session? 

## Session Sizes

The plot above tells a little bit about session sizes, but a histogram is probably a better way to visualize the distribution.

```{r}
ggplot(data, aes(x = hits)) + geom_histogram(binwidth = 25)
```

This confirms what we already saw in the last plot. The vast majority of sessions include fewer than 100 clicks. However, this histogram shows the count of sessions, not the count of clicks that belong to those sessions. 

```{r}
ggplot(data, aes(x = hits, weight = hits)) + geom_histogram(binwidth=25)
```

Conversely, this chart shows a weighted histogram where the size of each of the bins is relative to the number of hits, not sessions. If we consider that each click contains a certain amount of information, then if we choose to normalize all of our sessions to the same weighting, we throw away a lot of the information from larger sessions and favor the clicks from smaller sessions.
