---
title: "More PCA"
output: html_notebook
---

Last time I tried some basic PCA where we normalized each search session into a single "appetite" row of our adjacency matrix. The down side to this approach is that each session is given the same weighting in the results regardless of it's size. This means that 400 click sesssions have the same effect on our output as 4 click sessions. 

We don't really have that much data -- in particular, we don't have that many search sessions. In order to simulate more sessions and get as much as we can out of the data that we have, I'd like to try breaking the larger sessions up into smaller subsessions. 

> Theoretically this might even make a lot of sense given that one's appetite likely changes after looking at a lot of images of food -- so your appetite after clicking on 300 images of food might be quite different than it was when you started clicking on the first image in your search session. 

I'd also like to start including the time dimension. Unfortunately this will be a noisy variable because the time that it takes to click on an image will be effected by the network latency (how long it takes for the set of images to download) and any background distractions. As long as the noise isn't too great though, I do think there's some information available in the time that it takes to click -- annecdotally, I've definitely found that I take longer to decide which image to click on when the choice isn't obvious. 

## Initial Feature Engineering

Once again, we'll start with our standard imported access log dataframe:

```{r, message=FALSE}
source("prep-data.R")
```

In order to add in the time component (the time between clicks), we'll need to sort the requests by session and timestamp.


```{r}
library(dplyr)
data <- select(access_log, search.session, datetime, chosen, not.chosen) %>% arrange(., search.session, datetime)

threshold <- 60 * 20 ## force a new session after 20 minutes of inactivity
#data$datetime <- as.integer(data$datetime)
subsession.size <- 5

sdata <- split(data, data$search.session)
sdata <- lapply(sdata,function (x) {
    x$decision.time <- x$datetime - x$datetime[c(NA,1:nrow(x)-1)];
    x$median.time <- median(x$decision.time,na.rm=TRUE)
    x$subsession <- paste(x$search.session,as.integer(1:nrow(x)/subsession.size),sep=":")
    x;
  })
data <- do.call("rbind", sdata)

#data$decision.time <- as.integer(data$datetime) - as.integer(data$datetime[shifted])
# remove the erroneous times calculated for first click of a session
#data$decision.time[i[!data$same.session]] <- NA

#last.click.time <- append(NA, data$datetime[1:length(data$datetime)-1])
#last.click.session <- append(NA, data$search.session[1:length(data$search.session)-1])
```

A somewhat sloppier, but simpler and more scalable approach might just be to break up the world of time into discrete buckets -- for instance one bucket for ever clock minute -- and only include clicks that occurred in the same time bucket as part of the same session. This would give us variable sized buckets, and it would have the downside of sometimes breaking up very short sessions into two even shorter sessions, but it would guarantee us that every subsession would be *bite-sized* and only include clicks that happened close together in time (and close together in the visitors attention span/mood)

```{r}
library(dplyr)
data <- select(access_log, search.session, datetime, chosen, not.chosen) %>% arrange(., search.session, datetime)

bucket.size <- 60 ## sixty second/one minute buckets
data$subsession <- paste(data$search.session, as.integer(as.integer(data$datetime)/bucket.size), sep=":")

sdata <- split(data, data$subsession)
sdata <- lapply(sdata,function (x) {
    x$decision.time <- as.integer(x$datetime - x$datetime[c(NA,1:nrow(x)-1)]);
    mean <- mean(x$decision.time,na.rm=TRUE)
    mean <- if(is.na(mean)) 4 else mean
    x$decision.time[is.na(x$decision.time)] <- mean
    sd <- sd(x$decision.time,na.rm=TRUE)
    collar <- if(is.na(sd)) .5 else max(sd/2,.6)
    x$weight <- cut(x$decision.time,c(-Inf,mean-collar,mean+collar,Inf),labels=c(2,1,.5))
    x;
  })
data <- do.call("rbind", sdata)
```
One of the problems that I can immediately see with this approach is that while it simplifies the calculation of `decision.time` (we don't have to worry about scrubbing outliers), we also get a lot more NAs (the decision time for the first click of every time bucket is no longer available).

A hybrid approach is probably the best option, but I'm just going to keep going with this for now. 

The second thing that I'm noticing has to do with our lack of subsecond level time granularity and rounding error. Every one of our clicks has a data and time down to the second leve, but not the millisecond level. That means that if it only took half a second to make a decision and click on an image, we'll either see that real world response time as zero or one.

I originally thought that I would select a pivot point like mean or median decision time for a given session, and then weight each click by dividing that pivot point by the time that it took to make each decision. That way typical weightings would be close to one, decisions that took a while to make would be closer to zero and decisions that were made very quickly would have high weightings (we'd have to do something to avoid dividing by zero obviously). With the rounding error though, if we have a session with lots of clicks that are one or two seconds apart, it doesn't make sense to weight the 1 second decision times significantly more than the 2 second ones. We're probably better off using a discrete ranges and mapping them to hand picked weightings -- like .5 for long decision times, 2 for short ones and 1 for normal ones. 


## Building the Adjacency Matrix

```{r}
library(reshape2)
melted <- melt(data, 
               id.vars=c("subsession", "weight"),
               measure.vars=c("chosen", "not.chosen"))

## assign chosen and not chosen positive and negative weightings
map <- c(-1,1)
names(map) <- c("not.chosen","chosen")
melted$weight <- map[as.character(melted$variable)] * as.numeric(as.character(melted$weight))

ratings <- dcast(melted, subsession ~ value, fun.aggregate = sum, value.var="weight")
ratings.matrix <- as.matrix(ratings[,-1])
rownames(ratings.matrix) <- ratings[,1]

```
## Decompose the Matrix

We use the same SVD method from last time:

```{r}
dvu <- svd(ratings.matrix)
dvu$d[1:10]
```

### Dimension 1 

```{r}
rownames(dvu$v) <- colnames(ratings.matrix)
v <- dvu$v[,1]
ordered.ratings <- v[order(v)]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```


#### Top 10

```{r}
library(knitr)
## Note, this assumes that the images are sitting in a folder of the parent 
## directory named images
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Bottom 10


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```

### Dimension 2

```{r}
v <- dvu$v[,2]
ordered.ratings <- v[order(v)]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```


#### Top 10

```{r}
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Bottom 10


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```

### Dimension 3

```{r}
v <- dvu$v[,3]
ordered.ratings <- v[order(v)]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```


#### Top 10

```{r}
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Bottom 10


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```

### Dimension 4

```{r}
v <- dvu$v[,4]
ordered.ratings <- v[order(v)]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```


#### Top 10

```{r}
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Bottom 10


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```

### Dimension 5

```{r}
v <- dvu$v[,5]
ordered.ratings <- v[order(v)]
top10 <- tail(names(ordered.ratings),10)
bottom10 <- head(names(ordered.ratings),10)
```


#### Top 10

```{r}
top10.images=paste("../images/",top10,sep = "")
include_graphics(top10.images)
```

### Bottom 10


```{r}
bottom10.images=paste("../images/",bottom10,sep = "")
include_graphics(bottom10.images)
```


